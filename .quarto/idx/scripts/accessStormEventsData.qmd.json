{"title":"Extreme Weather Events Data: Access & Preprocessing","markdown":{"yaml":{"title":"Extreme Weather Events Data: Access & Preprocessing","author":"Emmanuel Guizar Rosales","date":"2024-08-16","date-format":"[last rendered on:] MMM D, YYYY","format":{"html":{"toc":true,"toc-depth":5,"toc-expand":2,"number-sections":true,"code-fold":true,"code-summary":"Show the code"}},"editor":"visual","execute":{"include":true,"echo":true,"message":false,"warning":false,"cache":true},"editor_options":{"chunk_output_type":"console"},"params":{"currentYear":2023},"bibliography":"references_accessStormEventsData.bib"},"headingText":"install package librarian if needed","containsRefs":false,"markdown":"\n\n```{r}\n#| label: setup\n\nif (!(\"librarian\" %in% rownames(installed.packages()))) {\n  install.packages(\"librarian\")\n}\n\n# load required packages\nlibrarian::shelf(\n  ropensci/rnoaa,\n  openxlsx,\n  tidyverse,\n  DT,\n  usmap,\n  sessioninfo\n)\n```\n\n# Access Data\n\n## Download Storm Events Data\n\nWe will use the [Storm Events Database](https://www.ncdc.noaa.gov/stormevents/) operated by the US National Oceanic and Atmospheric Administration. Full documentation regarding this database can be found [here](https://www.nws.noaa.gov/directives/sym/pd01016005curr.pdf). A detailed variable codebook is found [here](https://www.ncei.noaa.gov/pub/data/swdi/stormevents/csvfiles/Storm-Data-Bulk-csv-Format.pdf).\n\nThere is a handy R package that allows to download different NOAA data products: [rnoaa](https://github.com/ropensci/rnoaa). We use the functions `se_files()` and `se_data`.\n\nWe first want to get a feeling for all available files:\n\n```{r}\n#| label: showFiles_se_files\n\n# read in meta data of available files\nse_availableFiles <- se_files()\n\n# get an overview of metadata\ncat(\"Structure of meta data:\\n\")\nstr(se_availableFiles)\n\n# display unique file types\ncat(\"\\nUnique file types:\\n\")\nunique(se_availableFiles$type)\n```\n\nWe will mostly be interested in \"details\", \"locations\", and (maybe) \"fatalities\". What is the most recent data available for these files?\n\n```{r}\n#| label: showRecentFiles\n\nse_availableFiles %>% \n  filter(year > 2020) %>% \n  arrange(desc(year)) %>% \n  select(-url) %>% \n  knitr::kable()\n```\n\nNow, let's read in \"details\", \"locations\", and \"fatalities for the years `r params$currentYear - 10` up to `r params$currentYear`\n\n```{r}\n#| label: readInStormData\n#| message: false\n#| eval: false\n\n# define simple function (putting code in fuctions prevents workspace from being\n# clutered with variables we will not use again)\nread_in_storm_data <- function(\n    years_to_read_in = seq(params$currentYear - 10, params$currentYear, 1),\n    types_to_read_in = c(\"details\", \"locations\", \"fatalities\")\n    ) {\n  \n  # initialize data_list\n  data_list <- vector(\"list\", length = length(years_to_read_in))\n  names(data_list) <- years_to_read_in\n  for (year in seq(1, length(years_to_read_in))) {\n    data_list[[year]] <- vector(\"list\", length = length(types_to_read_in))\n    names(data_list[[year]]) <- as.character(types_to_read_in)\n  }\n  \n  # read in data over nested loop\n  for (i in seq(1,length(years_to_read_in))) {\n    for (j in seq(1,length(types_to_read_in))) {\n      currentData <- se_data(year = years_to_read_in[i], type = types_to_read_in[j])\n      data_list[[i]][[j]] <- currentData\n    }\n  }\n  \n  # save data_list\n  time <- format(Sys.time(), \"%Y%m%d\")\n  fileName <- paste0(time, \"_data_list.RDS\")\n  saveRDS(data_list, file = file.path(\"../data/stormData\", fileName))\n  # to read in data, use:\n  # data_list <- readRDS(\"../0_Data/data_list.RDS\")\n  \n  return(data_list)\n}\n\n# call function and store results in data_list\ndata_list <- read_in_storm_data()\n```\n\n```{r}\n#| label: readInStormData_fromSaved\n\n# load most recent data_list file\nfileName <- \"data_list.RDS\"\npathName <- \"../data/stormData\"\nfilePath <- fs::dir_ls(path = pathName, regexp = paste0(fileName, \"$\")) %>% last()\ndata_list <- readRDS(filePath)\n```\n\n## Inspect Data Structures\n\nWhat is the structure of \"details\", \"locations\", and \"fatalities\"?\n\n### Details\n\n```{r showStructureDetails}\n#| label: showStructureDetails\ndata_list[[as.character(params$currentYear)]]$details %>% \n  select(-c(episode_narrative, event_narrative)) %>% \n  head(.,10) %>% \n  datatable(options = list(scrollY = \"300px\"), fillContainer = TRUE)\n```\n\nNote: two additional variables are not included in the table above:\n\n-   `episode_narrative` (example:) `r data_list[[as.character(params$currentYear)]]$details$episode_narrative[1]`\n-   `event_narrative` (example:) `r data_list[[as.character(params$currentYear)]]$details$event_narrative[1]`\n\n### Locations\n\n```{r}\n#| label: showStructureLocations\n\ndata_list[[as.character(params$currentYear)]]$locations %>% \n  head(.,10) %>% \n  datatable(options = list(scrollY = \"300px\"), fillContainer = TRUE)\n```\n\n### Fatalities\n\n```{r}\n#| label: showStructureFatalities\n\ndata_list[[as.character(params$currentYear)]]$fatalities %>% \n  head(.,10) %>% \n  datatable(options = list(scrollY = \"300px\"), fillContainer = TRUE)\n```\n\n## Recency of Data\n\nWhat is the most recent data available in \"details\" for `params$currentYear`?\n\n```{r}\n#| label: recencyDetails\n\ndata_list[[as.character(params$currentYear)]]$details %>% \n  mutate(arrange_date = strptime(end_date_time, format = \"%d-%b-%y %H:%M:%S\") %>% as_datetime()) %>% \n  arrange(desc(arrange_date)) %>% \n  .$end_date_time %>% .[1]\n```\n\n## Create Combined Datasets\n\n```{r}\n#| label: createCombinedDatasets\n#| eval: false\n\ndata_details <- tibble()\nfor (i in seq(1, length(data_list))) {\n  data_details <- rbind(data_details, data_list[[i]]$details)\n}\n\ndata_locations <- tibble()\nfor (i in seq(1, length(data_list))) {\n  data_locations <- rbind(data_locations, data_list[[i]]$locations)\n}\n\ndata_fatalities <- tibble()\nfor (i in seq(1, length(data_list))) {\n  data_fatalities <- rbind(data_fatalities, data_list[[i]]$fatalities)\n}\n\n# since we will mostly work with data_details, let's save this data set\ntime <- format(Sys.time(), \"%Y%m%d\")\nfileName <- paste0(time, \"_data_details.RDS\")\nsaveRDS(data_details, file = file.path(\"../data/stormData\", fileName))\n```\n\n# Tidy Data\n\n## Simple Tidying\n\nWe will mainly work with `data_details`. Let's tidy up this data (convert some integers to characters, some characters to integers, and some characters to date-time format).\n\n```{r}\n#| label: tidyUpDataDetails\n\n# load most recent data_details\nfileName <- \"data_details.RDS\"\npathName <- \"../data/stormData\"\nfilePath <- fs::dir_ls(path = pathName, regexp = paste0(fileName, \"$\")) %>% last()\ndata_details <- readRDS(filePath)\n\n# convert some integers to characters so that they are not mistakenly treated\n# as integers\ntoChar <- c(\n  \"episode_id\", \"event_id\",\n  \"state_fips\", \"cz_fips\",\n  \"category\",\n  \"tor_other_cz_fips\"\n)\ndata_details <- data_details %>% \n  mutate(across(all_of(toChar), as.character))\nrm(toChar)\n\n# for damage values, we need to convert strings like\n# \"3.12M\" and \"117.00K\" to integer values. We define a function to do so:\nconvert_to_integer <- function(x) {\n  multiplier <- c(\"K\" = 1000, \"M\" = 1000000)\n  numeric_part <- as.numeric(sub(\"[^0-9.]\", \"\", x))\n  multiplier_part <- substr(x, nchar(x), nchar(x))\n  multiplier_value <- multiplier[multiplier_part]\n  return(as.integer(numeric_part * multiplier_value))\n}\n# apply function to \"damage\" variables\ndata_details <- data_details %>% \n  mutate(across(contains(\"damage\"), convert_to_integer))\nrm(convert_to_integer)\n\n# convert some character columns to date-time format\ndata_details <- data_details %>% \n  mutate(across(contains(\"date_time\"), dmy_hms))\n\n# store month_name as factor with the correct order of months as levels\ndata_details <- data_details %>% \n  mutate(month_name = factor(month_name, levels = month.name))\n\n# state_fips need to be two digits long. If the first digit was zero, this leading\n# zero was removed during the read in process of the data. This is corrected\n# using str_pad. Similarly, cz_fips needs to be three digits long.\ndata_details <- data_details %>% \n  mutate(state_fips = str_pad(state_fips, width = 2, side = \"left\", pad = \"0\")) %>% \n  mutate(cz_fips = str_pad(cz_fips, width = 3, side = \"left\", pad = \"0\"))\n```\n\n## FIPS Tidying\n\nFor various forms of analyses, we will need the Federal Information Processing System (FIPS) Codes for States and Counties. For instance, we will need this information to associate specific extreme weather events with certain geographical regions (mostly Counties). The package [usmap](https://github.com/pdil/usmap), which we will use to display geographical distributions of extreme weather events, works with county FIPS. Therefore, we need to tidy up our data to show events that conform to such FIPS data. The FIPS information is stored in two separate variables in the data set: `state_fips` and `cz_fips`.\n\n**Note that the storm event database assigns FIPS not only to County/Parish (`cz_type == \"C\"`), but also NWS Public Forecast Zone and Marine Zones (`cz_type == \"Z\"`).**\n\nThus, the meaning of variable `cz_fips` depends on `cz_type`. Therefore, we first need to convert Z-type FIPS to C-type FIPS. There is a [mapping](https://www.nws.noaa.gov/directives/sym/pd01005007curr.pdf) of Forecast Zones onto County FIPS we can use for this. In the following code block, we\n\n1.  create a data set `mappingData` with all the information to map NWS Forecast Zones to County FIPS and\n\n2.  use this mapping information to create a variable `county_fips` representing the County FIPS and\n\n3.  ultimately create a variable `state_county_fips` representing the full FIPS identifying each County in each State.\n\n```{r}\n#| label: FUNConvertFips\n#| eval: false\n\nFUNConvertFips <- function(myData) {\n  \n  # define vector of urls to .txt files containing the mapping information\n  urlsToFiles <- c(\n    \"https://www.weather.gov/source/pimar/PubRep/PUB_AR.txt\",\n    \"https://www.weather.gov/source/pimar/PubRep/PUB_CR.txt\",\n    \"https://www.weather.gov/source/pimar/PubRep/PUB_ER.txt\",\n    \"https://www.weather.gov/source/pimar/PubRep/PUB_PR.txt\",\n    \"https://www.weather.gov/source/pimar/PubRep/PUB_SR.txt\",\n    \"https://www.weather.gov/source/pimar/PubRep/PUB_WR.txt\"\n  )\n  \n  # define the variable names of each column in these files\n  varNames <- c(\n    \"state_abr\",\n    \"id_zone\",\n    \"location_descr\",\n    \"county\",\n    \"fips\",\n    \"city\",\n    \"state_city_abr\"\n  )\n  \n  # define a function to read in the files\n  FUNReadFiles <- function(myURL) {\n    dataOut <- read.table(\n      file = myURL,\n      sep = \"|\",\n      header = FALSE,\n      quote = \"\",\n      colClasses = \"character\",\n      fill = FALSE\n    )\n    names(dataOut) <- varNames\n    return(dataOut)\n  }\n  \n  # \"loop\" over this function and continuously combine read in data into one data frame\n  mappingData <- plyr::ldply(urlsToFiles, FUNReadFiles) %>% \n    as_tibble()\n  \n  # create data set that maps state abbreviations to full state names\n  mapping_states_abbr <- usmap::fips_info() %>%\n    as_tibble() %>% \n    select(abbr, full) %>% \n    mutate(state = toupper(full)) %>% \n    select(-full)\n  \n  # combine mappingData with mapping_states_abbr\n  mappingData <- mappingData %>% \n    left_join(y = mapping_states_abbr, by = c(\"state_abr\" = \"abbr\")) %>% \n    relocate(state, .after = state_abr) %>% \n    mutate(cz_fips = id_zone) %>% \n    relocate(cz_fips, .after = id_zone) %>% \n    mutate(county_fips = str_extract(fips, \".{3}$\")) %>% \n    relocate(county_fips, .after = fips)\n  \n  # subset myData to myData_czTypeC and myData_czTypeZ\n  myData_czTypeC <- myData %>% \n    filter(cz_type == \"C\")\n  myData_czTypeZ <- myData %>% \n    filter(cz_type == \"Z\")\n  \n  # define county_fips in myData_czTypeC\n  myData_czTypeC <- myData_czTypeC %>% \n    mutate(county_fips = cz_fips)\n  \n  # define county_fips in myData_czTypeZ\n  # first, create a temporary dataset\n  tmp <- left_join(\n    x = myData_czTypeZ,\n    y = mappingData %>% \n      select(state, cz_fips, county_fips),\n    by = c(\"state\", \"cz_fips\"),\n    relationship = \"many-to-many\"\n  )\n  # then, create a list with meta information for myData_czTypeZ\n  myData_czTypeZ_meta <- list(\n    county_fips_isNA = tmp %>% filter(is.na(county_fips)),\n    event_id_isDoublicated = tmp %>% filter(duplicated(event_id)),\n    myData_czTypeZ_raw = tmp,\n    myData_czTypeZ_county_fips_isNA.removed = tmp %>% filter(!is.na(county_fips))\n  )\n  # define myData_czTypeZ\n  # note that we only retain events whose county_fips is not NA! To see where\n  # these NAs come from, see myData_czTypeZ_meta$county_fips_isNA\n  myData_czTypeZ <- myData_czTypeZ_meta$myData_czTypeZ_county_fips_isNA.removed\n  \n  # join myData_czTypeC and myData_czTypeZ\n  if (all(names(myData_czTypeC) == names(myData_czTypeZ))) {\n    myData_fips <- rbind(\n      myData_czTypeC,\n      myData_czTypeZ\n    )\n  } else {\n    stop(\"all(names(myData_czTypeC) == names(myData_czTypeZ)) != TRUE\")\n  }\n  \n  # create variable state_county_fips representing the full and correct fips for each county\n  myData_fips <- myData_fips %>% \n    mutate(state_county_fips = str_c(state_fips, county_fips))\n  \n  return(list(\n    myData_fips = myData_fips,\n    myData_czTypeZ_meta = myData_czTypeZ_meta\n  ))\n  \n}\n\n# call function\nout_FUNConvertFips <- FUNConvertFips(data_details)\n\n# define data_details_fips\ndata_details_fips <- out_FUNConvertFips$myData_fips\n\n# save data_details_fips\ntime <- format(Sys.time(), \"%Y%m%d\")\nfileName <- paste0(time, \"_data_details_fips_raw.RDS\")\nsaveRDS(data_details_fips, file = file.path(\"../data/stormData\", fileName))\n```\n\nAfter these changes, some challenges remain, which become obvious, if we try to match the FIPS provided in the data set with current FIPS available in `usmap`.\n\n```{r}\n#| label: identifyProblemsWithFips\n\n# load most recent data_details_fips\nfileName <- \"data_details_fips_raw.RDS\"\npathName <- \"../data/stormData\"\nfilePath <- fs::dir_ls(path = pathName, regexp = paste0(fileName, \"$\")) %>% last()\ndata_details_fips <- readRDS(filePath)\n\n# capture the warning message produced if we apply usmap::fips_info to\n# all unique state_county_fips in data_details_fips\nwarning_message <- tryCatch({\n  result <- fips_info(fips = unique(as.character(data_details_fips$state_county_fips)))\n}, warning = function(w) {\n  return(conditionMessage(w))\n})\nunmatched_fips <- str_extract_all(warning_message, \"\\\\b\\\\d{5}\\\\b\")[[1]] %>% unique()\n\n# create a tibble containing all unmatched fips\nunmatched_fips <- tibble(\n  state_county_fips = unmatched_fips,\n  state_fips = state_county_fips %>% \n    str_extract(\"^.{2}\"),\n  county_fips = state_county_fips %>% \n    str_extract(\".{3}$\")\n) %>% \n  arrange(state_county_fips)\n\n# in which states do we find unmatched fips?\nunmatched_fips_states <- unmatched_fips %>% \n  distinct(state_fips) %>% \n  left_join(\n    y = usmap::fips_info(),\n    by =c(\"state_fips\" = \"fips\")\n  )\n\n# # inspect the main land states that had unmatched fips\n# unmatched_fips_states_mainLand <- unmatched_fips_states %>% \n#   filter(!is.na(abbr))\n# plot_usmap(regions = \"counties\", include = unmatched_fips_states_mainLand$abbr[1], labels = TRUE)\n# plot_usmap(regions = \"counties\", include = unmatched_fips_states_mainLand$abbr[2], labels = TRUE)\n# unmatched_fips %>%\n#   filter(state_fips == \"02\")\n# \n# data_details_fips %>% \n#   filter(state_fips %in% (unmatched_fips_states %>% \n#            filter(is.na(abbr)) %>% \n#            pull(state_fips))) %>% \n#   select(state, cz_name) %>% \n#   distinct(state)\n```\n\n```{r}\n#| label: tbl-unmatchedFips\n#| tbl-cap: FIPS in data_details_fips that do not match with current FIPS as provided by `usmap`.\n\ndatatable(unmatched_fips)\n```\n\n```{r}\n#| label: tbl-unmatchedFipsStates\n#| tbl-cap: States associated with the unmatched FIPS reported in @tbl-unmatchedFips.\n\nknitr::kable(unmatched_fips_states)\n```\n\nWhat is the origin of these unmatched FIPS? First, as listed in @tbl-terretoriesOfUS, `state_fips` `r seq(96, 99, 1)` are assigned to the following states that are considered territories (and not states) of the US. We will exclude these territories in further analyses.\n\n```{r}\n#| label: tbl-terretoriesOfUS\n#| tbl-cap: Terretories of the US in the `data_details_fips`\n\ndata_details_fips %>%\n  filter(state_fips %in% (unmatched_fips_states %>%\n           filter(is.na(abbr)) %>%\n           pull(state_fips))) %>%\n  distinct(state_fips, .keep_all = TRUE) %>% \n  select(state_fips, state) %>% \n  knitr::kable()\n```\n\nSecond, there are many unmatched FIPS in the state of Alaska (@tbl-unmatchedFipsAlaska). This is due to the fact that Alaska had substantial changes to counties and their boundaries over the years (see [here](https://www.census.gov/programs-surveys/geography/technical-documentation/county-changes.2010.html#list-tab-957819518)). Consistent information on how to map old counties to new counties is not readily available. We will exclude Alaska from further analyses.\n\n```{r}\n#| label: tbl-unmatchedFipsAlaska\n#| tbl-cap: Unmatched FIPS in the state of Alaska.\n\ndata_details_fips %>% \n  filter(state_county_fips %in% filter(unmatched_fips, state_fips == \"02\")$state_county_fips) %>% \n  select(state_county_fips, state, cz_name) %>% \n  distinct() %>% \n  datatable()\n\n# # are there unmatched FIPS for Alaska in the latest year data? - YES\n# data_details_fips %>% \n#   filter(year == params$currentYear, state_fips == \"02\") %>% \n#   filter(state_county_fips %in% unmatched_fips$state_county_fips) %>% \n#   select(state_county_fips, state, cz_name)\n```\n\nThird, there were substantial changes in the state of Connecticut: Originally, it consisted of 8 counties. After the changes, these counties were replaced with 9 new counties (for more information, see [here](https://www.census.gov/programs-surveys/geography/technical-documentation/county-changes.2020.html#list-tab-957819518) and [here](https://www.federalregister.gov/documents/2022/06/06/2022-12063/change-to-county-equivalents-in-the-state-of-connecticut)). The old counties map to the new counties according to @tbl-connecticutNewPlanningReagions-fineGrained.\n\n```{r}\n#| label: tbl-connecticutNewPlanningReagions-fineGrained\n#| tbl-cap: Connecticut exact mapping of old counties to new counties.\n\n# read in xlsx file mapping old counties and FIPS to new counties and FIPS\nconnecticut_newPlanningRegions <- openxlsx::read.xlsx(\n  xlsxFile = \"https://www2.census.gov/geo/docs/reference/ct_change/ct_cou_to_cousub_crosswalk.xlsx\"\n) %>% \n  as_tibble() %>% \n  select(c(1:5))\n\n# rename variable names\nnames(connecticut_newPlanningRegions) <- c(\n  \"state_fips\",\n  \"old_county_fips\",\n  \"old_county_name\",\n  \"new_county_fips\",\n  \"new_county_name\"\n)\n\n# select distinct combinations of our variables of interest\nconnecticut_newPlanningRegions <- connecticut_newPlanningRegions %>% \n  distinct(state_fips, old_county_fips, new_county_fips, .keep_all = TRUE)\n\n# combine state and county fips\nconnecticut_newPlanningRegions <- connecticut_newPlanningRegions %>% \n  mutate(state_county_fips_old = paste0(state_fips, old_county_fips),\n         state_county_fips_new = paste0(state_fips, new_county_fips))\n\n# display table\ndatatable(connecticut_newPlanningRegions)\n```\n\nHowever, this mapping is too fine grained for our purposes. A better mapping can be achieved following @fig-connecticutOldToNewCountiesMapping, which results in the following mapping summarized in @tbl-connecticutNewPlanningReagions-lessFineGrained.\n\n![Mapping of old to new counties in Connecticut.](https://img.federalregister.gov/EN06JN22.001/EN06JN22.001_original_size.png){#fig-connecticutOldToNewCountiesMapping}\n\n```{r}\n#| label: tbl-connecticutNewPlanningReagions-lessFineGrained\n#| tbl-cap: Connecticut mapping of old counties to new counties.\n\n\n# https://www.federalregister.gov/documents/2022/06/06/2022-12063/change-to-county-equivalents-in-the-state-of-connecticut\n\nold_fips <- unmatched_fips %>% \n  filter(state_fips == \"09\") %>% \n  arrange(state_county_fips) %>% \n  pull(state_county_fips)\nold_counties <- c(\n  \"Fairfield\",\n  \"Harford\",\n  \"Litchfield\",\n  \"Middlesex\",\n  \"New Haven\",\n  \"New London\",\n  \"Tolland\",\n  \"Windham\"\n)\nold <- tibble(\n  old_county = old_counties,\n  old_fips = old_fips\n)\n\nnew_counties <- c(\n  \"Greater Bridgeport Planning Region\",\n  \"Western Connecticut Planning Region\",\n  \"Capitol Planning Region\",\n  \"Northwest Hills Planning Region\",\n  \"Lower Connecticut River Valley Planning Region\",\n  \"Naugatuck Valley Planning Region\",\n  \"South Central Connecticut Planning Region\",\n  \"Southeastern Connecticut Planning Region\",\n  \"Northeastern Connecticut Planning Region\"\n)\nnew_fips <- usmap::fips(state = \"CT\", county = new_counties)\nnew <- tibble(\n  new_county = new_counties,\n  new_fips = new_fips\n)\n\nold_counties_to_new_counties <- tibble(\n  old_county = c(\n    \"Fairfield\",\n    \"Fairfield\",\n    \"Harford\",\n    \"Tolland\",\n    \"Litchfield\",\n    \"Middlesex\",\n    \"New Haven\",\n    \"New Haven\",\n    \"New London\",\n    \"Windham\"\n  ),\n  new_county = c(\n    \"Greater Bridgeport Planning Region\",\n    \"Western Connecticut Planning Region\",\n    \"Capitol Planning Region\",\n    \"Capitol Planning Region\",\n    \"Northwest Hills Planning Region\",\n    \"Lower Connecticut River Valley Planning Region\",\n    \"Naugatuck Valley Planning Region\",\n    \"South Central Connecticut Planning Region\",\n    \"Southeastern Connecticut Planning Region\",\n    \"Northeastern Connecticut Planning Region\"\n  )\n)\n\nconnecticut_newPlanningRegions <- left_join(\n  x = old_counties_to_new_counties,\n  y = old,\n  by = \"old_county\"\n) %>% \n  left_join(\n    y = new,\n    by = \"new_county\"\n  )\n\n# do some renamign and add new_county_fips\nconnecticut_newPlanningRegions <- connecticut_newPlanningRegions %>% \n  rename(\n    old_state_county_fips = old_fips,\n    new_state_county_fips = new_fips\n  ) %>% \n  mutate(new_county_fips = new_state_county_fips %>% str_extract(\".{3}$\"))\n\ndatatable(connecticut_newPlanningRegions)\n```\n\nNow we apply the changes mentioned above to `data_details_fips`:\n\n1.  remove US territories from data set\n\n2.  remove state of Alaska from data set\n\n3.  apply new county names and FIPS to the state of Connecticut\n\n```{r}\n#| label: applyAdditionalFipsChanges\n#| eval: false\n\n# we apply the changes outlined in the main text in slightly changed order\n\n# first, we apply the new county names and FIPS to the state of Connecticut\n\n# We check whether all state_county_fips for Connecticut are recorded using the old\n# fips.\ntmp <- data_details_fips %>% \n  filter(state_fips == \"09\") %>% \n  filter(!state_county_fips %in% unmatched_fips$state_county_fips)\n\nif (!nrow(tmp)) {\n  message(\"All state_county_fips are recorded following the old FIPS codes.\")\n} else {\n  warning(\"There are state_county_fips that are recorded following the new FIPS codes!\")\n}\nrm(tmp)\n\n# create a data set without Connecticut entries that follow the old FIPS codes\ndata_details_fips_withoutCT <- data_details_fips %>% \n  filter(!(state_fips == \"09\" & (state_county_fips %in% unmatched_fips$state_county_fips)))\n\n# create a data set containing only Connecticut entries that follow the old FIPS codes\ndata_details_fips_CT <- data_details_fips %>% \n  filter(state_fips == \"09\" & (state_county_fips %in% unmatched_fips$state_county_fips))\n\n# mutate county_fips and state_county_fips so that they represent the new county_fips\n# and state_county_fips. Note that this will inflate the previously data_details_fips_CT\n# because there is a one-to-many relationsipt between old and new fips.\ndata_details_fips_CT <- left_join(\n    x = data_details_fips_CT,\n    y = connecticut_newPlanningRegions %>% \n      select(old_state_county_fips, new_state_county_fips, new_county_fips),\n    by = c(\"state_county_fips\" = \"old_state_county_fips\")\n  ) %>% \n  mutate(\n    county_fips = new_county_fips,\n    state_county_fips = new_state_county_fips\n  ) %>% \n  select(-starts_with(\"new_\"))\n\n# check whether the datasets without and with Connecticut have the same structure before\n# joning them again.\nif (all(names(data_details_fips_withoutCT) == names(data_details_fips_CT))) {\n  message(\"all(names(data_details_fips_withoutCT) == names(data_details_fips_CT)) == TRUE\")\n  data_details_fips <- bind_rows(\n    data_details_fips_withoutCT,\n    data_details_fips_CT\n  )\n} else {\n  warning(\"The names of data_details_fips_withoutCT and data_details_fips_CT do not matach!\")\n}\n\n# then, we remove the US territories from the data set\ndata_details_fips <- data_details_fips %>%\n  filter(!state_fips %in% (unmatched_fips_states %>%\n                             filter(is.na(abbr)) %>%\n                             pull(state_fips)))\n\n# finally, we remove the state of Alaska (FIPS = 02) from the data set, but we save\n# a data set still containing Alaska just in case...\ndata_details_fips_withAK <- data_details_fips\n\ndata_details_fips <- data_details_fips %>%\n  filter(state_fips != \"02\")\n```\n\nFinally, we rearrange some variables in `data_details_fips` and save them for later use. We also store `state_fips`, `state_county_fips` and `event_type` as factors. This becomes handy for accurately counting number of events by groups later on.\n\n```{r}\n#| label: tidyUpDataDetailsFips\n#| eval: false\n\n# rearrange order of variables in the data set\ndata_details_fips <- data_details_fips %>% \n  select(\n    begin_yearmonth,\n    episode_id, event_id,\n    state, state_county_fips,\n    event_type,\n    starts_with(\"damage\"),\n    starts_with(\"injuries\"),\n    starts_with(\"death\"),\n    everything()\n  ) %>% \n  arrange(begin_yearmonth, episode_id, event_id, state_county_fips)\n\n# store state_county_fips and event_type as factor\ndata_details_fips <- data_details_fips %>% \n  mutate(\n    state_fips = factor(state_fips, levels = sort(unique(.$state_fips))),\n    state_county_fips = factor(state_county_fips, levels = sort(unique(.$state_county_fips))),\n    event_type = factor(event_type, levels = sort(unique(.$event_type)))\n  )\n\n# save data_details_fips\ntime <- format(Sys.time(), \"%Y%m%d\")\nfileName <- paste0(time, \"_data_details_fips.RDS\")\nsaveRDS(data_details_fips, file = file.path(\"../data/stormData\", fileName))\n```\n\n::: {.callout-note collapse=\"true\" appearance=\"simple\"}\n## Expand for Session Info\n\n```{r, echo = FALSE}\n#| label: addSessionInfo\n#| echo: false\n\n# Save the session info as an object\npkg_sesh <- session_info(pkgs = \"attached\")\n\n# Get the quarto version\nquarto_version <- system(\"quarto --version\", intern = TRUE)\n\n# Inject the quarto info\npkg_sesh$platform$quarto <- paste(\n  system(\"quarto --version\", intern = TRUE), \n  \"@\", \n  quarto::quarto_path()\n  )\n\n# Print it out\npkg_sesh\n```\n:::\n","srcMarkdownNoYaml":"\n\n```{r}\n#| label: setup\n\n# install package librarian if needed\nif (!(\"librarian\" %in% rownames(installed.packages()))) {\n  install.packages(\"librarian\")\n}\n\n# load required packages\nlibrarian::shelf(\n  ropensci/rnoaa,\n  openxlsx,\n  tidyverse,\n  DT,\n  usmap,\n  sessioninfo\n)\n```\n\n# Access Data\n\n## Download Storm Events Data\n\nWe will use the [Storm Events Database](https://www.ncdc.noaa.gov/stormevents/) operated by the US National Oceanic and Atmospheric Administration. Full documentation regarding this database can be found [here](https://www.nws.noaa.gov/directives/sym/pd01016005curr.pdf). A detailed variable codebook is found [here](https://www.ncei.noaa.gov/pub/data/swdi/stormevents/csvfiles/Storm-Data-Bulk-csv-Format.pdf).\n\nThere is a handy R package that allows to download different NOAA data products: [rnoaa](https://github.com/ropensci/rnoaa). We use the functions `se_files()` and `se_data`.\n\nWe first want to get a feeling for all available files:\n\n```{r}\n#| label: showFiles_se_files\n\n# read in meta data of available files\nse_availableFiles <- se_files()\n\n# get an overview of metadata\ncat(\"Structure of meta data:\\n\")\nstr(se_availableFiles)\n\n# display unique file types\ncat(\"\\nUnique file types:\\n\")\nunique(se_availableFiles$type)\n```\n\nWe will mostly be interested in \"details\", \"locations\", and (maybe) \"fatalities\". What is the most recent data available for these files?\n\n```{r}\n#| label: showRecentFiles\n\nse_availableFiles %>% \n  filter(year > 2020) %>% \n  arrange(desc(year)) %>% \n  select(-url) %>% \n  knitr::kable()\n```\n\nNow, let's read in \"details\", \"locations\", and \"fatalities for the years `r params$currentYear - 10` up to `r params$currentYear`\n\n```{r}\n#| label: readInStormData\n#| message: false\n#| eval: false\n\n# define simple function (putting code in fuctions prevents workspace from being\n# clutered with variables we will not use again)\nread_in_storm_data <- function(\n    years_to_read_in = seq(params$currentYear - 10, params$currentYear, 1),\n    types_to_read_in = c(\"details\", \"locations\", \"fatalities\")\n    ) {\n  \n  # initialize data_list\n  data_list <- vector(\"list\", length = length(years_to_read_in))\n  names(data_list) <- years_to_read_in\n  for (year in seq(1, length(years_to_read_in))) {\n    data_list[[year]] <- vector(\"list\", length = length(types_to_read_in))\n    names(data_list[[year]]) <- as.character(types_to_read_in)\n  }\n  \n  # read in data over nested loop\n  for (i in seq(1,length(years_to_read_in))) {\n    for (j in seq(1,length(types_to_read_in))) {\n      currentData <- se_data(year = years_to_read_in[i], type = types_to_read_in[j])\n      data_list[[i]][[j]] <- currentData\n    }\n  }\n  \n  # save data_list\n  time <- format(Sys.time(), \"%Y%m%d\")\n  fileName <- paste0(time, \"_data_list.RDS\")\n  saveRDS(data_list, file = file.path(\"../data/stormData\", fileName))\n  # to read in data, use:\n  # data_list <- readRDS(\"../0_Data/data_list.RDS\")\n  \n  return(data_list)\n}\n\n# call function and store results in data_list\ndata_list <- read_in_storm_data()\n```\n\n```{r}\n#| label: readInStormData_fromSaved\n\n# load most recent data_list file\nfileName <- \"data_list.RDS\"\npathName <- \"../data/stormData\"\nfilePath <- fs::dir_ls(path = pathName, regexp = paste0(fileName, \"$\")) %>% last()\ndata_list <- readRDS(filePath)\n```\n\n## Inspect Data Structures\n\nWhat is the structure of \"details\", \"locations\", and \"fatalities\"?\n\n### Details\n\n```{r showStructureDetails}\n#| label: showStructureDetails\ndata_list[[as.character(params$currentYear)]]$details %>% \n  select(-c(episode_narrative, event_narrative)) %>% \n  head(.,10) %>% \n  datatable(options = list(scrollY = \"300px\"), fillContainer = TRUE)\n```\n\nNote: two additional variables are not included in the table above:\n\n-   `episode_narrative` (example:) `r data_list[[as.character(params$currentYear)]]$details$episode_narrative[1]`\n-   `event_narrative` (example:) `r data_list[[as.character(params$currentYear)]]$details$event_narrative[1]`\n\n### Locations\n\n```{r}\n#| label: showStructureLocations\n\ndata_list[[as.character(params$currentYear)]]$locations %>% \n  head(.,10) %>% \n  datatable(options = list(scrollY = \"300px\"), fillContainer = TRUE)\n```\n\n### Fatalities\n\n```{r}\n#| label: showStructureFatalities\n\ndata_list[[as.character(params$currentYear)]]$fatalities %>% \n  head(.,10) %>% \n  datatable(options = list(scrollY = \"300px\"), fillContainer = TRUE)\n```\n\n## Recency of Data\n\nWhat is the most recent data available in \"details\" for `params$currentYear`?\n\n```{r}\n#| label: recencyDetails\n\ndata_list[[as.character(params$currentYear)]]$details %>% \n  mutate(arrange_date = strptime(end_date_time, format = \"%d-%b-%y %H:%M:%S\") %>% as_datetime()) %>% \n  arrange(desc(arrange_date)) %>% \n  .$end_date_time %>% .[1]\n```\n\n## Create Combined Datasets\n\n```{r}\n#| label: createCombinedDatasets\n#| eval: false\n\ndata_details <- tibble()\nfor (i in seq(1, length(data_list))) {\n  data_details <- rbind(data_details, data_list[[i]]$details)\n}\n\ndata_locations <- tibble()\nfor (i in seq(1, length(data_list))) {\n  data_locations <- rbind(data_locations, data_list[[i]]$locations)\n}\n\ndata_fatalities <- tibble()\nfor (i in seq(1, length(data_list))) {\n  data_fatalities <- rbind(data_fatalities, data_list[[i]]$fatalities)\n}\n\n# since we will mostly work with data_details, let's save this data set\ntime <- format(Sys.time(), \"%Y%m%d\")\nfileName <- paste0(time, \"_data_details.RDS\")\nsaveRDS(data_details, file = file.path(\"../data/stormData\", fileName))\n```\n\n# Tidy Data\n\n## Simple Tidying\n\nWe will mainly work with `data_details`. Let's tidy up this data (convert some integers to characters, some characters to integers, and some characters to date-time format).\n\n```{r}\n#| label: tidyUpDataDetails\n\n# load most recent data_details\nfileName <- \"data_details.RDS\"\npathName <- \"../data/stormData\"\nfilePath <- fs::dir_ls(path = pathName, regexp = paste0(fileName, \"$\")) %>% last()\ndata_details <- readRDS(filePath)\n\n# convert some integers to characters so that they are not mistakenly treated\n# as integers\ntoChar <- c(\n  \"episode_id\", \"event_id\",\n  \"state_fips\", \"cz_fips\",\n  \"category\",\n  \"tor_other_cz_fips\"\n)\ndata_details <- data_details %>% \n  mutate(across(all_of(toChar), as.character))\nrm(toChar)\n\n# for damage values, we need to convert strings like\n# \"3.12M\" and \"117.00K\" to integer values. We define a function to do so:\nconvert_to_integer <- function(x) {\n  multiplier <- c(\"K\" = 1000, \"M\" = 1000000)\n  numeric_part <- as.numeric(sub(\"[^0-9.]\", \"\", x))\n  multiplier_part <- substr(x, nchar(x), nchar(x))\n  multiplier_value <- multiplier[multiplier_part]\n  return(as.integer(numeric_part * multiplier_value))\n}\n# apply function to \"damage\" variables\ndata_details <- data_details %>% \n  mutate(across(contains(\"damage\"), convert_to_integer))\nrm(convert_to_integer)\n\n# convert some character columns to date-time format\ndata_details <- data_details %>% \n  mutate(across(contains(\"date_time\"), dmy_hms))\n\n# store month_name as factor with the correct order of months as levels\ndata_details <- data_details %>% \n  mutate(month_name = factor(month_name, levels = month.name))\n\n# state_fips need to be two digits long. If the first digit was zero, this leading\n# zero was removed during the read in process of the data. This is corrected\n# using str_pad. Similarly, cz_fips needs to be three digits long.\ndata_details <- data_details %>% \n  mutate(state_fips = str_pad(state_fips, width = 2, side = \"left\", pad = \"0\")) %>% \n  mutate(cz_fips = str_pad(cz_fips, width = 3, side = \"left\", pad = \"0\"))\n```\n\n## FIPS Tidying\n\nFor various forms of analyses, we will need the Federal Information Processing System (FIPS) Codes for States and Counties. For instance, we will need this information to associate specific extreme weather events with certain geographical regions (mostly Counties). The package [usmap](https://github.com/pdil/usmap), which we will use to display geographical distributions of extreme weather events, works with county FIPS. Therefore, we need to tidy up our data to show events that conform to such FIPS data. The FIPS information is stored in two separate variables in the data set: `state_fips` and `cz_fips`.\n\n**Note that the storm event database assigns FIPS not only to County/Parish (`cz_type == \"C\"`), but also NWS Public Forecast Zone and Marine Zones (`cz_type == \"Z\"`).**\n\nThus, the meaning of variable `cz_fips` depends on `cz_type`. Therefore, we first need to convert Z-type FIPS to C-type FIPS. There is a [mapping](https://www.nws.noaa.gov/directives/sym/pd01005007curr.pdf) of Forecast Zones onto County FIPS we can use for this. In the following code block, we\n\n1.  create a data set `mappingData` with all the information to map NWS Forecast Zones to County FIPS and\n\n2.  use this mapping information to create a variable `county_fips` representing the County FIPS and\n\n3.  ultimately create a variable `state_county_fips` representing the full FIPS identifying each County in each State.\n\n```{r}\n#| label: FUNConvertFips\n#| eval: false\n\nFUNConvertFips <- function(myData) {\n  \n  # define vector of urls to .txt files containing the mapping information\n  urlsToFiles <- c(\n    \"https://www.weather.gov/source/pimar/PubRep/PUB_AR.txt\",\n    \"https://www.weather.gov/source/pimar/PubRep/PUB_CR.txt\",\n    \"https://www.weather.gov/source/pimar/PubRep/PUB_ER.txt\",\n    \"https://www.weather.gov/source/pimar/PubRep/PUB_PR.txt\",\n    \"https://www.weather.gov/source/pimar/PubRep/PUB_SR.txt\",\n    \"https://www.weather.gov/source/pimar/PubRep/PUB_WR.txt\"\n  )\n  \n  # define the variable names of each column in these files\n  varNames <- c(\n    \"state_abr\",\n    \"id_zone\",\n    \"location_descr\",\n    \"county\",\n    \"fips\",\n    \"city\",\n    \"state_city_abr\"\n  )\n  \n  # define a function to read in the files\n  FUNReadFiles <- function(myURL) {\n    dataOut <- read.table(\n      file = myURL,\n      sep = \"|\",\n      header = FALSE,\n      quote = \"\",\n      colClasses = \"character\",\n      fill = FALSE\n    )\n    names(dataOut) <- varNames\n    return(dataOut)\n  }\n  \n  # \"loop\" over this function and continuously combine read in data into one data frame\n  mappingData <- plyr::ldply(urlsToFiles, FUNReadFiles) %>% \n    as_tibble()\n  \n  # create data set that maps state abbreviations to full state names\n  mapping_states_abbr <- usmap::fips_info() %>%\n    as_tibble() %>% \n    select(abbr, full) %>% \n    mutate(state = toupper(full)) %>% \n    select(-full)\n  \n  # combine mappingData with mapping_states_abbr\n  mappingData <- mappingData %>% \n    left_join(y = mapping_states_abbr, by = c(\"state_abr\" = \"abbr\")) %>% \n    relocate(state, .after = state_abr) %>% \n    mutate(cz_fips = id_zone) %>% \n    relocate(cz_fips, .after = id_zone) %>% \n    mutate(county_fips = str_extract(fips, \".{3}$\")) %>% \n    relocate(county_fips, .after = fips)\n  \n  # subset myData to myData_czTypeC and myData_czTypeZ\n  myData_czTypeC <- myData %>% \n    filter(cz_type == \"C\")\n  myData_czTypeZ <- myData %>% \n    filter(cz_type == \"Z\")\n  \n  # define county_fips in myData_czTypeC\n  myData_czTypeC <- myData_czTypeC %>% \n    mutate(county_fips = cz_fips)\n  \n  # define county_fips in myData_czTypeZ\n  # first, create a temporary dataset\n  tmp <- left_join(\n    x = myData_czTypeZ,\n    y = mappingData %>% \n      select(state, cz_fips, county_fips),\n    by = c(\"state\", \"cz_fips\"),\n    relationship = \"many-to-many\"\n  )\n  # then, create a list with meta information for myData_czTypeZ\n  myData_czTypeZ_meta <- list(\n    county_fips_isNA = tmp %>% filter(is.na(county_fips)),\n    event_id_isDoublicated = tmp %>% filter(duplicated(event_id)),\n    myData_czTypeZ_raw = tmp,\n    myData_czTypeZ_county_fips_isNA.removed = tmp %>% filter(!is.na(county_fips))\n  )\n  # define myData_czTypeZ\n  # note that we only retain events whose county_fips is not NA! To see where\n  # these NAs come from, see myData_czTypeZ_meta$county_fips_isNA\n  myData_czTypeZ <- myData_czTypeZ_meta$myData_czTypeZ_county_fips_isNA.removed\n  \n  # join myData_czTypeC and myData_czTypeZ\n  if (all(names(myData_czTypeC) == names(myData_czTypeZ))) {\n    myData_fips <- rbind(\n      myData_czTypeC,\n      myData_czTypeZ\n    )\n  } else {\n    stop(\"all(names(myData_czTypeC) == names(myData_czTypeZ)) != TRUE\")\n  }\n  \n  # create variable state_county_fips representing the full and correct fips for each county\n  myData_fips <- myData_fips %>% \n    mutate(state_county_fips = str_c(state_fips, county_fips))\n  \n  return(list(\n    myData_fips = myData_fips,\n    myData_czTypeZ_meta = myData_czTypeZ_meta\n  ))\n  \n}\n\n# call function\nout_FUNConvertFips <- FUNConvertFips(data_details)\n\n# define data_details_fips\ndata_details_fips <- out_FUNConvertFips$myData_fips\n\n# save data_details_fips\ntime <- format(Sys.time(), \"%Y%m%d\")\nfileName <- paste0(time, \"_data_details_fips_raw.RDS\")\nsaveRDS(data_details_fips, file = file.path(\"../data/stormData\", fileName))\n```\n\nAfter these changes, some challenges remain, which become obvious, if we try to match the FIPS provided in the data set with current FIPS available in `usmap`.\n\n```{r}\n#| label: identifyProblemsWithFips\n\n# load most recent data_details_fips\nfileName <- \"data_details_fips_raw.RDS\"\npathName <- \"../data/stormData\"\nfilePath <- fs::dir_ls(path = pathName, regexp = paste0(fileName, \"$\")) %>% last()\ndata_details_fips <- readRDS(filePath)\n\n# capture the warning message produced if we apply usmap::fips_info to\n# all unique state_county_fips in data_details_fips\nwarning_message <- tryCatch({\n  result <- fips_info(fips = unique(as.character(data_details_fips$state_county_fips)))\n}, warning = function(w) {\n  return(conditionMessage(w))\n})\nunmatched_fips <- str_extract_all(warning_message, \"\\\\b\\\\d{5}\\\\b\")[[1]] %>% unique()\n\n# create a tibble containing all unmatched fips\nunmatched_fips <- tibble(\n  state_county_fips = unmatched_fips,\n  state_fips = state_county_fips %>% \n    str_extract(\"^.{2}\"),\n  county_fips = state_county_fips %>% \n    str_extract(\".{3}$\")\n) %>% \n  arrange(state_county_fips)\n\n# in which states do we find unmatched fips?\nunmatched_fips_states <- unmatched_fips %>% \n  distinct(state_fips) %>% \n  left_join(\n    y = usmap::fips_info(),\n    by =c(\"state_fips\" = \"fips\")\n  )\n\n# # inspect the main land states that had unmatched fips\n# unmatched_fips_states_mainLand <- unmatched_fips_states %>% \n#   filter(!is.na(abbr))\n# plot_usmap(regions = \"counties\", include = unmatched_fips_states_mainLand$abbr[1], labels = TRUE)\n# plot_usmap(regions = \"counties\", include = unmatched_fips_states_mainLand$abbr[2], labels = TRUE)\n# unmatched_fips %>%\n#   filter(state_fips == \"02\")\n# \n# data_details_fips %>% \n#   filter(state_fips %in% (unmatched_fips_states %>% \n#            filter(is.na(abbr)) %>% \n#            pull(state_fips))) %>% \n#   select(state, cz_name) %>% \n#   distinct(state)\n```\n\n```{r}\n#| label: tbl-unmatchedFips\n#| tbl-cap: FIPS in data_details_fips that do not match with current FIPS as provided by `usmap`.\n\ndatatable(unmatched_fips)\n```\n\n```{r}\n#| label: tbl-unmatchedFipsStates\n#| tbl-cap: States associated with the unmatched FIPS reported in @tbl-unmatchedFips.\n\nknitr::kable(unmatched_fips_states)\n```\n\nWhat is the origin of these unmatched FIPS? First, as listed in @tbl-terretoriesOfUS, `state_fips` `r seq(96, 99, 1)` are assigned to the following states that are considered territories (and not states) of the US. We will exclude these territories in further analyses.\n\n```{r}\n#| label: tbl-terretoriesOfUS\n#| tbl-cap: Terretories of the US in the `data_details_fips`\n\ndata_details_fips %>%\n  filter(state_fips %in% (unmatched_fips_states %>%\n           filter(is.na(abbr)) %>%\n           pull(state_fips))) %>%\n  distinct(state_fips, .keep_all = TRUE) %>% \n  select(state_fips, state) %>% \n  knitr::kable()\n```\n\nSecond, there are many unmatched FIPS in the state of Alaska (@tbl-unmatchedFipsAlaska). This is due to the fact that Alaska had substantial changes to counties and their boundaries over the years (see [here](https://www.census.gov/programs-surveys/geography/technical-documentation/county-changes.2010.html#list-tab-957819518)). Consistent information on how to map old counties to new counties is not readily available. We will exclude Alaska from further analyses.\n\n```{r}\n#| label: tbl-unmatchedFipsAlaska\n#| tbl-cap: Unmatched FIPS in the state of Alaska.\n\ndata_details_fips %>% \n  filter(state_county_fips %in% filter(unmatched_fips, state_fips == \"02\")$state_county_fips) %>% \n  select(state_county_fips, state, cz_name) %>% \n  distinct() %>% \n  datatable()\n\n# # are there unmatched FIPS for Alaska in the latest year data? - YES\n# data_details_fips %>% \n#   filter(year == params$currentYear, state_fips == \"02\") %>% \n#   filter(state_county_fips %in% unmatched_fips$state_county_fips) %>% \n#   select(state_county_fips, state, cz_name)\n```\n\nThird, there were substantial changes in the state of Connecticut: Originally, it consisted of 8 counties. After the changes, these counties were replaced with 9 new counties (for more information, see [here](https://www.census.gov/programs-surveys/geography/technical-documentation/county-changes.2020.html#list-tab-957819518) and [here](https://www.federalregister.gov/documents/2022/06/06/2022-12063/change-to-county-equivalents-in-the-state-of-connecticut)). The old counties map to the new counties according to @tbl-connecticutNewPlanningReagions-fineGrained.\n\n```{r}\n#| label: tbl-connecticutNewPlanningReagions-fineGrained\n#| tbl-cap: Connecticut exact mapping of old counties to new counties.\n\n# read in xlsx file mapping old counties and FIPS to new counties and FIPS\nconnecticut_newPlanningRegions <- openxlsx::read.xlsx(\n  xlsxFile = \"https://www2.census.gov/geo/docs/reference/ct_change/ct_cou_to_cousub_crosswalk.xlsx\"\n) %>% \n  as_tibble() %>% \n  select(c(1:5))\n\n# rename variable names\nnames(connecticut_newPlanningRegions) <- c(\n  \"state_fips\",\n  \"old_county_fips\",\n  \"old_county_name\",\n  \"new_county_fips\",\n  \"new_county_name\"\n)\n\n# select distinct combinations of our variables of interest\nconnecticut_newPlanningRegions <- connecticut_newPlanningRegions %>% \n  distinct(state_fips, old_county_fips, new_county_fips, .keep_all = TRUE)\n\n# combine state and county fips\nconnecticut_newPlanningRegions <- connecticut_newPlanningRegions %>% \n  mutate(state_county_fips_old = paste0(state_fips, old_county_fips),\n         state_county_fips_new = paste0(state_fips, new_county_fips))\n\n# display table\ndatatable(connecticut_newPlanningRegions)\n```\n\nHowever, this mapping is too fine grained for our purposes. A better mapping can be achieved following @fig-connecticutOldToNewCountiesMapping, which results in the following mapping summarized in @tbl-connecticutNewPlanningReagions-lessFineGrained.\n\n![Mapping of old to new counties in Connecticut.](https://img.federalregister.gov/EN06JN22.001/EN06JN22.001_original_size.png){#fig-connecticutOldToNewCountiesMapping}\n\n```{r}\n#| label: tbl-connecticutNewPlanningReagions-lessFineGrained\n#| tbl-cap: Connecticut mapping of old counties to new counties.\n\n\n# https://www.federalregister.gov/documents/2022/06/06/2022-12063/change-to-county-equivalents-in-the-state-of-connecticut\n\nold_fips <- unmatched_fips %>% \n  filter(state_fips == \"09\") %>% \n  arrange(state_county_fips) %>% \n  pull(state_county_fips)\nold_counties <- c(\n  \"Fairfield\",\n  \"Harford\",\n  \"Litchfield\",\n  \"Middlesex\",\n  \"New Haven\",\n  \"New London\",\n  \"Tolland\",\n  \"Windham\"\n)\nold <- tibble(\n  old_county = old_counties,\n  old_fips = old_fips\n)\n\nnew_counties <- c(\n  \"Greater Bridgeport Planning Region\",\n  \"Western Connecticut Planning Region\",\n  \"Capitol Planning Region\",\n  \"Northwest Hills Planning Region\",\n  \"Lower Connecticut River Valley Planning Region\",\n  \"Naugatuck Valley Planning Region\",\n  \"South Central Connecticut Planning Region\",\n  \"Southeastern Connecticut Planning Region\",\n  \"Northeastern Connecticut Planning Region\"\n)\nnew_fips <- usmap::fips(state = \"CT\", county = new_counties)\nnew <- tibble(\n  new_county = new_counties,\n  new_fips = new_fips\n)\n\nold_counties_to_new_counties <- tibble(\n  old_county = c(\n    \"Fairfield\",\n    \"Fairfield\",\n    \"Harford\",\n    \"Tolland\",\n    \"Litchfield\",\n    \"Middlesex\",\n    \"New Haven\",\n    \"New Haven\",\n    \"New London\",\n    \"Windham\"\n  ),\n  new_county = c(\n    \"Greater Bridgeport Planning Region\",\n    \"Western Connecticut Planning Region\",\n    \"Capitol Planning Region\",\n    \"Capitol Planning Region\",\n    \"Northwest Hills Planning Region\",\n    \"Lower Connecticut River Valley Planning Region\",\n    \"Naugatuck Valley Planning Region\",\n    \"South Central Connecticut Planning Region\",\n    \"Southeastern Connecticut Planning Region\",\n    \"Northeastern Connecticut Planning Region\"\n  )\n)\n\nconnecticut_newPlanningRegions <- left_join(\n  x = old_counties_to_new_counties,\n  y = old,\n  by = \"old_county\"\n) %>% \n  left_join(\n    y = new,\n    by = \"new_county\"\n  )\n\n# do some renamign and add new_county_fips\nconnecticut_newPlanningRegions <- connecticut_newPlanningRegions %>% \n  rename(\n    old_state_county_fips = old_fips,\n    new_state_county_fips = new_fips\n  ) %>% \n  mutate(new_county_fips = new_state_county_fips %>% str_extract(\".{3}$\"))\n\ndatatable(connecticut_newPlanningRegions)\n```\n\nNow we apply the changes mentioned above to `data_details_fips`:\n\n1.  remove US territories from data set\n\n2.  remove state of Alaska from data set\n\n3.  apply new county names and FIPS to the state of Connecticut\n\n```{r}\n#| label: applyAdditionalFipsChanges\n#| eval: false\n\n# we apply the changes outlined in the main text in slightly changed order\n\n# first, we apply the new county names and FIPS to the state of Connecticut\n\n# We check whether all state_county_fips for Connecticut are recorded using the old\n# fips.\ntmp <- data_details_fips %>% \n  filter(state_fips == \"09\") %>% \n  filter(!state_county_fips %in% unmatched_fips$state_county_fips)\n\nif (!nrow(tmp)) {\n  message(\"All state_county_fips are recorded following the old FIPS codes.\")\n} else {\n  warning(\"There are state_county_fips that are recorded following the new FIPS codes!\")\n}\nrm(tmp)\n\n# create a data set without Connecticut entries that follow the old FIPS codes\ndata_details_fips_withoutCT <- data_details_fips %>% \n  filter(!(state_fips == \"09\" & (state_county_fips %in% unmatched_fips$state_county_fips)))\n\n# create a data set containing only Connecticut entries that follow the old FIPS codes\ndata_details_fips_CT <- data_details_fips %>% \n  filter(state_fips == \"09\" & (state_county_fips %in% unmatched_fips$state_county_fips))\n\n# mutate county_fips and state_county_fips so that they represent the new county_fips\n# and state_county_fips. Note that this will inflate the previously data_details_fips_CT\n# because there is a one-to-many relationsipt between old and new fips.\ndata_details_fips_CT <- left_join(\n    x = data_details_fips_CT,\n    y = connecticut_newPlanningRegions %>% \n      select(old_state_county_fips, new_state_county_fips, new_county_fips),\n    by = c(\"state_county_fips\" = \"old_state_county_fips\")\n  ) %>% \n  mutate(\n    county_fips = new_county_fips,\n    state_county_fips = new_state_county_fips\n  ) %>% \n  select(-starts_with(\"new_\"))\n\n# check whether the datasets without and with Connecticut have the same structure before\n# joning them again.\nif (all(names(data_details_fips_withoutCT) == names(data_details_fips_CT))) {\n  message(\"all(names(data_details_fips_withoutCT) == names(data_details_fips_CT)) == TRUE\")\n  data_details_fips <- bind_rows(\n    data_details_fips_withoutCT,\n    data_details_fips_CT\n  )\n} else {\n  warning(\"The names of data_details_fips_withoutCT and data_details_fips_CT do not matach!\")\n}\n\n# then, we remove the US territories from the data set\ndata_details_fips <- data_details_fips %>%\n  filter(!state_fips %in% (unmatched_fips_states %>%\n                             filter(is.na(abbr)) %>%\n                             pull(state_fips)))\n\n# finally, we remove the state of Alaska (FIPS = 02) from the data set, but we save\n# a data set still containing Alaska just in case...\ndata_details_fips_withAK <- data_details_fips\n\ndata_details_fips <- data_details_fips %>%\n  filter(state_fips != \"02\")\n```\n\nFinally, we rearrange some variables in `data_details_fips` and save them for later use. We also store `state_fips`, `state_county_fips` and `event_type` as factors. This becomes handy for accurately counting number of events by groups later on.\n\n```{r}\n#| label: tidyUpDataDetailsFips\n#| eval: false\n\n# rearrange order of variables in the data set\ndata_details_fips <- data_details_fips %>% \n  select(\n    begin_yearmonth,\n    episode_id, event_id,\n    state, state_county_fips,\n    event_type,\n    starts_with(\"damage\"),\n    starts_with(\"injuries\"),\n    starts_with(\"death\"),\n    everything()\n  ) %>% \n  arrange(begin_yearmonth, episode_id, event_id, state_county_fips)\n\n# store state_county_fips and event_type as factor\ndata_details_fips <- data_details_fips %>% \n  mutate(\n    state_fips = factor(state_fips, levels = sort(unique(.$state_fips))),\n    state_county_fips = factor(state_county_fips, levels = sort(unique(.$state_county_fips))),\n    event_type = factor(event_type, levels = sort(unique(.$event_type)))\n  )\n\n# save data_details_fips\ntime <- format(Sys.time(), \"%Y%m%d\")\nfileName <- paste0(time, \"_data_details_fips.RDS\")\nsaveRDS(data_details_fips, file = file.path(\"../data/stormData\", fileName))\n```\n\n::: {.callout-note collapse=\"true\" appearance=\"simple\"}\n## Expand for Session Info\n\n```{r, echo = FALSE}\n#| label: addSessionInfo\n#| echo: false\n\n# Save the session info as an object\npkg_sesh <- session_info(pkgs = \"attached\")\n\n# Get the quarto version\nquarto_version <- system(\"quarto --version\", intern = TRUE)\n\n# Inject the quarto info\npkg_sesh$platform$quarto <- paste(\n  system(\"quarto --version\", intern = TRUE), \n  \"@\", \n  quarto::quarto_path()\n  )\n\n# Print it out\npkg_sesh\n```\n:::\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":true,"freeze":false,"echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"message":false,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"toc":true,"toc-depth":5,"number-sections":true,"output-file":"accessStormEventsData.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.45","editor":"visual","theme":"cosmo","title":"Extreme Weather Events Data: Access & Preprocessing","author":"Emmanuel Guizar Rosales","date":"2024-08-16","date-format":"[last rendered on:] MMM D, YYYY","editor_options":{"chunk_output_type":"console"},"params":{"currentYear":2023},"bibliography":["references_accessStormEventsData.bib"],"toc-expand":2,"code-summary":"Show the code"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}